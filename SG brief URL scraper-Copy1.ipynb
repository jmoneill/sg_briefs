{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code to scrape the Department of Justice's website to download all the briefs filed by the Solicitor General within a particular time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As set up right now, this will search for briefs from specified October Terms. The Supreme Court divides up its years into October Terms, which is simply the term that starts with the Court's first October sitting (where they're coming back from a lengthy summer vacation). So searching for briefs from \"2005\" will return briefs that were filed during the year starting in October 2005. (OT 2005 was a particular exciting term, as it was the first term of Chief Justice Roberts and Justice Alito!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_links = [] ## initialize list\n",
    "\n",
    "year_start = 2000  \n",
    "year_stop = 2017   ## replace these with the years you want to stop and search, respectively. \n",
    "\n",
    "for i in list(range(year_start,year_stop)):\n",
    "    pdf_count = 1  ## hacky way to make sure the loop below actually starts\n",
    "    counter = 0\n",
    "    print(i) ## this just prints whatever year the link scraper is on so you can keep track of the progress\n",
    "    while pdf_count > 0: ## stops the scraper when it reaches a page with no PDF links - reached end of search results\n",
    "        url = \"https://www.justice.gov/osg/supreme-court-briefs?text=&sc_term=\" + str(i) + \"&type=All&subject=All&filing_date%5Bvalue%5D%5Bmonth%5D=&filing_date%5Bvalue%5D%5Byear%5D=&page=\" + str(counter)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html5lib')\n",
    "        loop_links = soup.find_all(\"a\", string=re.compile(\"pdf\")) ## find all links to PDF documents on the page\n",
    "        loop_urls = [u['href'] for u in loop_links] ## extract just the href tag in the links\n",
    "        pdf_count = len(loop_urls) ## how many PDF URL's did the scraper pull this round of the for loop?\n",
    "        pdf_links = pdf_links + loop_urls ## add the links to the master link list\n",
    "        counter += 1 ## makes sure the next loop goes to the next page of search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_links) ## how many results did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = \"urls.txt\"\n",
    "\n",
    "with open(file_name, \"wb\") as fp:   #saves the list of URL's to a text file\n",
    "    pickle.dump(pdf_links, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
